{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a79b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook illustrates the complete analysis process from preparing input files\n",
    "# to calculating impact and novelty scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99e8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preparing input files\n",
    "\n",
    "# In its raw format, the input file contains the text of one patent file per line.\n",
    "# Each line starts with a path pointing to that patent's original text \n",
    "# file (\"/Volumes/External/txt/0000000-0100000/US1009.txt\"), followed by the patent text. \n",
    "\n",
    "# The included input-file contains a small subset of the patent data, for test purposes only.\n",
    "# The year file contains the year of publication of each patent.\n",
    "# The CPC-file (Cooperative Patent Classification) contains patent classification codes.\n",
    "# These codes are used to calculate benchmark similarities.\n",
    "\n",
    "# Note: the included data files (raw_input.txt, year.csv GPCPCs.txt) only contain a small\n",
    "# subset of the original data, for example purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1048ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_file = Path(\"./data/raw_input.txt\")\n",
    "year_file = Path(\"./data/year.csv\")\n",
    "cpc_fp = Path(\"./data/GPCPCs.txt\")\n",
    "patent_dir = Path(\"./patents\")\n",
    "output_fp = Path(\"./output\", \"patents.h5\")\n",
    "results_fp = Path(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docembedder.preprocessor.parser import compress_raw\n",
    "\n",
    "# The compressor function transforms the patents to a more manageable format,\n",
    "# sorts them by year of publication, and compresses the files. \n",
    "\n",
    "if len([path for path in patent_dir.iterdir() if path.suffix==\".xz\"])==0:\n",
    "    print(\"Compressing raw files\")\n",
    "    patent_dir.mkdir(exist_ok=True)\n",
    "    compress_raw(input_file, year_file, patent_dir)\n",
    "else:\n",
    "    print(f\"xz-files already present in '{patent_dir}'\")    \n",
    "\n",
    "# You now have XZ-compressed files containing patents per year. Each file contains\n",
    "# a list of JSON-objects, each JSON-object has the following key/values:\n",
    "\n",
    "# - patent: patent's ID\n",
    "# - file: path of original text file (not actually used)\n",
    "# - contents: patent text\n",
    "# - year: year of publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4852ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculating embeddings\n",
    "\n",
    "# Each model has its own preprocessor with various parameters. Most models also have\n",
    "# configurable hyperparameters. The values for these parameters have been optimised\n",
    "# using the original dataset, resulting in the values used in the compute_embeddings()-function below.\n",
    "\n",
    "# To recalibrate preprocessor and model parameters, run each model's hyperopt-script. See the \n",
    "# hyperopt-notebooks (hyperopt/) for more details.\n",
    "\n",
    "from docembedder.simspec import SimulationSpecification\n",
    "from docembedder.models import TfidfEmbedder\n",
    "from docembedder.preprocessor.preprocessor import Preprocessor\n",
    "from docembedder.models.doc2vec import D2VEmbedder\n",
    "from docembedder.models import CountVecEmbedder\n",
    "from docembedder.models import BERTEmbedder\n",
    "\n",
    "from docembedder.utils import run_models\n",
    "from docembedder.pretrained_run import pretrained_run_models\n",
    "import datetime\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "def check_files(sim_spec):\n",
    "    for year in range(sim_spec.year_start, sim_spec.year_end):\n",
    "        if not (patent_dir / f\"{year}.xz\").is_file():\n",
    "            raise ValueError(f\"Please download patent file {year}.xz and put it in\"\n",
    "                             f\"the right directory ({patent_dir})\")\n",
    "\n",
    "def compute_embeddings_cv(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_cv = {\n",
    "        \"countvec\": CountVecEmbedder(method='sigmoid')\n",
    "    }\n",
    "    prep_cv = {\n",
    "        \"prep-countvec\": Preprocessor(keep_caps=False, keep_start_section=False, remove_non_alpha=True)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_cv, model_cv, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated countvec emdeddings')\n",
    "\n",
    "    \n",
    "def compute_embeddings_tfidf(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "    \n",
    "    model_tfidf = {\n",
    "        \"tfidf\": TfidfEmbedder(\n",
    "            ngram_max=1,stop_words='english',stem=True, norm='l1', sublinear_tf=True, min_df=6, max_df=0.665461)\n",
    "    }\n",
    "    prep_tfidf = {\n",
    "        \"prep-tfidf\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True),\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_tfidf, model_tfidf, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated tfidf emdeddings')\n",
    "\n",
    "    \n",
    "def compute_embeddings_doc2vec(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_doc2vec = {\n",
    "        \"doc2vec\": D2VEmbedder(epoch=9, min_count=7, vector_size=101)\n",
    "    }\n",
    "    prep_doc2vec = {\n",
    "        \"prep-doc2vec\": Preprocessor(keep_caps=False, keep_start_section=True, remove_non_alpha=False)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_doc2vec, model_doc2vec, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated doc2vec emdeddings')\n",
    "\n",
    "def compute_embeddings_bert(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_bert = {\n",
    "        \"bert\": BERTEmbedder(pretrained_model='AI-Growth-Lab/PatentSBERTa')\n",
    "    }\n",
    "    prep_bert = {\n",
    "         \"prep-bert\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    pretrained_run_models(prep_bert, model_bert, sim_spec, patent_dir, output_fp, cpc_fp)\n",
    "    print('Calculated BERT emdeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8123f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specifications for the calculation of the embeddings.\n",
    "# Note that the year_end itself is not included in the range\n",
    "sim_spec = SimulationSpecification(\n",
    "    year_start=1870,\n",
    "    year_end=1911,\n",
    "    window_size=11,\n",
    "    window_shift=1,\n",
    "#     debug_max_patents=100\n",
    ")\n",
    "\n",
    "# Number of concurrent jobs to run. A higher number means faster processing, but be aware\n",
    "# that each job takes utilises one CPU-core.\n",
    "jobs=2\n",
    "\n",
    "# Calculate embeddings using all four models: Countvec, TfIdf, Doc2Vec, BERT (PatentSBERTa)\n",
    "# Be aware, depending on the amlount of patents and window size, this will take quite some\n",
    "# time, and can require a (very) large amount of memory. \n",
    "# For testing, you can set the debug_max_patents-attribute of the SimulationSpecification to\n",
    "# restrict the number of patents per year.\n",
    "# (Warnings from the Countvec calculations can be ignored).\n",
    "args={'patent_dir': patent_dir, 'output_fp': output_fp, 'cpc_fp': cpc_fp, 'sim_spec': sim_spec, 'n_jobs': jobs}\n",
    "compute_embeddings_cv(**args)\n",
    "compute_embeddings_tfidf(**args)\n",
    "compute_embeddings_doc2vec(**args)\n",
    "compute_embeddings_bert(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculating impact and novelty scores\n",
    "\n",
    "from docembedder.analysis import DocAnalysis\n",
    "from docembedder.datamodel import DataModel\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def compute_impacts(embedding_fp, output_dir):\n",
    "    exponents = [1.0, 2.0, 3.0]\n",
    "\n",
    "    impact_novel = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    with DataModel(embedding_fp, read_only=False) as data:\n",
    "        analysis = DocAnalysis(data)\n",
    "       \n",
    "        for window, model in data.iterate_window_models():\n",
    "            results = analysis.impact_novelty_results(window, model, exponents, cache=False, n_jobs=8)\n",
    "\n",
    "            for expon, res in results.items():\n",
    "                if expon == exponents[0]:\n",
    "                    impact_novel[model][\"patent_ids\"].extend(res[\"patent_ids\"])\n",
    "                impact_novel[model][f\"impact-{expon}\"].extend(res[\"impact\"])\n",
    "                impact_novel[model][f\"novelty-{expon}\"].extend(res[\"novelty\"])\n",
    "\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for model, data in impact_novel.items():\n",
    "        classifier_name = model.split(\"-\")[-1]\n",
    "        impact_fp = Path(output_dir, f\"impact-{classifier_name}.csv\")\n",
    "        pd.DataFrame(impact_novel[model]).sort_values(\"patent_ids\").to_csv(impact_fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b151d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute novelty and impact scores and write them to file\n",
    "compute_impacts(embedding_fp=output_fp, output_dir=results_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result files\n",
    "[str(path.absolute()) for path in results_fp.iterdir()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
