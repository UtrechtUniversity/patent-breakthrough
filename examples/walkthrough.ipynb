{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2839077a",
   "metadata": {},
   "source": [
    "# Patent Breakthrough walkthrough\n",
    "\n",
    "This notebook illustrates the complete analysis process of breakthrough patents, from preparing input files\n",
    "to calculating impact and novelty scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e317e63",
   "metadata": {},
   "source": [
    "## 1. Preparing input files\n",
    "\n",
    "There are three input files: a file with patent texts, a patent/year-index, and list of patent/CPC-codes.\n",
    "\n",
    "### Patents\n",
    "In its raw format, the input file contains the text of one patent file per line.\n",
    "Each line starts with a path pointing to that patent's original text \n",
    "file (`/Volumes/External/txt/0000000-0100000/US1009.txt`), followed by the patent text. Example file: `./data/raw_input.txt`. \n",
    "\n",
    "\n",
    "### Patent/Year-index\n",
    "Contains the year of publication of each patent. Example file: `./data/year.csv`. \n",
    "\n",
    "\n",
    "### CPC-file\n",
    "The CPC-file (Cooperative Patent Classification) contains the patent classification code for each patent. These codes are used to calculate benchmark similarities. Example file: `./data/GPCPCs.txt`\n",
    "\n",
    "Note: the included data files only contain a small subset of the original data, for example purposes.\n",
    "\n",
    "#### Other files\n",
    "The three other files in the data folder - `greek.txt`  `stopwords.txt`, and `symbols.txt` - are required by the `OldPreprocessor`-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = \"./data\"\n",
    "input_file = Path(f\"{data_path}/raw_input.txt\")\n",
    "year_file = Path(f\"{data_path}/year.csv\")\n",
    "cpc_fp = Path(f\"{data_path}/GPCPCs.txt\")\n",
    "patent_dir = Path(\"./patents\")\n",
    "output_folder = Path(\"./output\")\n",
    "output_fp = Path(\"./output\", \"patents.h5\")\n",
    "results_fp = Path(\"./results\")\n",
    "\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "patent_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69345ab9",
   "metadata": {},
   "source": [
    "### 1.1. Compressing\n",
    "\n",
    "The compressor function transforms the patents to a more manageable format, sorts and saves them by year of publication, and compresses the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e9c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docembedder.preprocessor.parser import compress_raw\n",
    "\n",
    "\n",
    "if len([path for path in patent_dir.iterdir() if path.suffix==\".xz\"])==0:\n",
    "    print(\"Compressing raw files\")\n",
    "    compress_raw(input_file, year_file, patent_dir)\n",
    "else:\n",
    "    print(f\"xz-files already present in '{patent_dir}'\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2633cb",
   "metadata": {},
   "source": [
    "You now have XZ-compressed files containing patents per year. Each file contains a list of JSON-objects, each JSON-object has the following key/values:\n",
    "\n",
    "- `patent`: patent's ID\n",
    "- `file`: path of original text file (not actually used)\n",
    "- `contents`: patent text\n",
    "- `year`: year of publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50576ac",
   "metadata": {},
   "source": [
    "## 2. Calculating embeddings\n",
    "\n",
    "We calculate embeddings and scores with four different models: Countvec, Tf-Idf, Doc2Vec, and BERT ([PatentSBERTa](https://github.com/AI-Growth-Lab/PatentSBERTa)).\n",
    "\n",
    "\n",
    "### 2.1. Preprocessors & parameters\n",
    "Each model has its own preprocessor with various parameters. Most models also have configurable hyperparameters. The values for these parameters have been optimised using the original dataset, resulting in the values used in the `compute_embeddings()`-function below.\n",
    "\n",
    "To recalibrate preprocessor and model parameters, run each model's hyperopt-script. See the [readme](https://github.com/UtrechtUniversity/patent-breakthrough/blob/main/docs/hyperparameter.md) and [hyperopt-notebooks](hyperopt/) for more details.\n",
    "\n",
    "\n",
    "### 2.2. Calculating embeddings\n",
    "Next, we calculate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docembedder.models import TfidfEmbedder\n",
    "from docembedder.preprocessor.preprocessor import Preprocessor\n",
    "from docembedder.preprocessor.oldprep import OldPreprocessor\n",
    "from docembedder.models.doc2vec import D2VEmbedder\n",
    "from docembedder.models import CountVecEmbedder\n",
    "from docembedder.models import BERTEmbedder\n",
    "\n",
    "from docembedder.utils import run_models\n",
    "from docembedder.pretrained_run import pretrained_run_models\n",
    "import datetime\n",
    "\n",
    "def check_files(sim_spec):\n",
    "    for year in range(sim_spec.year_start, sim_spec.year_end):\n",
    "        if not (patent_dir / f\"{year}.xz\").is_file():\n",
    "            raise ValueError(f\"Please download patent file {year}.xz and put it in\"\n",
    "                             f\"the right directory ({patent_dir})\")\n",
    "\n",
    "def compute_embeddings_cv(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_cv = {\n",
    "        \"countvec\": CountVecEmbedder(method='sigmoid')\n",
    "    }\n",
    "    prep_cv = {\n",
    "        \"prep-countvec\": OldPreprocessor(list_path=data_path)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_cv, model_cv, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated countvec emdeddings')\n",
    "\n",
    "    \n",
    "def compute_embeddings_tfidf(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "    \n",
    "    model_tfidf = {\n",
    "        \"tfidf\": TfidfEmbedder(\n",
    "            ngram_max=1,stop_words='english',stem=False, norm='l1', sublinear_tf=True, min_df=6, max_df=0.665461)\n",
    "    }\n",
    "    prep_tfidf = {\n",
    "        \"prep-tfidf\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True),\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_tfidf, model_tfidf, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated tfidf emdeddings')\n",
    "\n",
    "    \n",
    "def compute_embeddings_doc2vec(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_doc2vec = {\n",
    "        \"doc2vec\": D2VEmbedder(epoch=8, min_count=13, vector_size=100)\n",
    "    }\n",
    "    prep_doc2vec = {\n",
    "        \"prep-doc2vec\": Preprocessor(keep_caps=False, keep_start_section=True, remove_non_alpha=False)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    run_models(prep_doc2vec, model_doc2vec, sim_spec, patent_dir, output_fp, cpc_fp, n_jobs=n_jobs)\n",
    "    print('Calculated doc2vec emdeddings')\n",
    "\n",
    "def compute_embeddings_bert(patent_dir, output_fp, cpc_fp, sim_spec, n_jobs):\n",
    "\n",
    "    model_bert = {\n",
    "        \"bert\": BERTEmbedder(pretrained_model='AI-Growth-Lab/PatentSBERTa')\n",
    "    }\n",
    "    prep_bert = {\n",
    "         \"prep-bert\": Preprocessor(keep_caps=True, keep_start_section=True, remove_non_alpha=True)\n",
    "    }\n",
    "\n",
    "    check_files(sim_spec)\n",
    "    pretrained_run_models(prep_bert, model_bert, sim_spec, patent_dir, output_fp, cpc_fp)\n",
    "    print('Calculated BERT emdeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f8df3",
   "metadata": {},
   "source": [
    "#### Defining the calculation window\n",
    "\n",
    "Embeddings are calculated within a time window, which shifts over the dataset and then recalculated.\n",
    "This procedure is configured with the `SimulationSpecification()`, which has the following attributes:\n",
    "    \n",
    "- `year_start`: start year of the entire (sub)set of data to calculate embeddings for.\n",
    "- `year_end`: id. end year (the end year itself is not included).\n",
    "- `window_size`: width of the window (in years) to compute embeddings for.\n",
    "- `window_shift`: number of years between subsequent windows.\n",
    "- `debug_max_patents`: restrict the number of patents per year (optional; for testing purposes).\n",
    "    \n",
    "With the `n_jobs`-parameter you can set the number of concurrent jobs to run. A higher number means faster processing, but be aware that each job takes utilises one CPU-core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8123f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docembedder.simspec import SimulationSpecification\n",
    "\n",
    "sim_spec = SimulationSpecification(\n",
    "    year_start=1877,\n",
    "    year_end=1897,\n",
    "    window_size=11,\n",
    "    window_shift=1\n",
    ")\n",
    "\n",
    "n_jobs=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42590c",
   "metadata": {},
   "source": [
    "#### Computing embeddings\n",
    "\n",
    "Now that we've defined the window, we can calculate embeddings, using each of the four models.\n",
    "    \n",
    "Be aware, depending on the amlount of patents and window size, this will take quite some time, \n",
    "and can require a (_very_) large amount of memory. Warnings from the Countvec calculations can be ignored.\n",
    "\n",
    "All output is stored in a HDF5 file, which contains embeddings for all patents in all windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61043e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={'patent_dir': patent_dir, 'output_fp': output_fp, 'cpc_fp': cpc_fp, 'sim_spec': sim_spec, 'n_jobs': n_jobs}\n",
    "\n",
    "# Countvec\n",
    "compute_embeddings_cv(**args)\n",
    "\n",
    "# Tf-Idf\n",
    "compute_embeddings_tfidf(**args)\n",
    "\n",
    "# Doc2Vec\n",
    "compute_embeddings_doc2vec(**args)\n",
    "\n",
    "# BERT\n",
    "compute_embeddings_bert(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b7efe",
   "metadata": {},
   "source": [
    "## 3. Impact and novelty scores\n",
    "\n",
    "### 3.1. Calculating the scores\n",
    "\n",
    "After we've computed and stored the embeddings, we compute novelty and impact scores. The result is a dictionary per model, each containing the novelties and impacts for each patent.\n",
    "\n",
    "\n",
    "_Note on exponents_\n",
    "\n",
    "The exponents (`[1.0, 2.0, 3.0]`) are used in the calculations to reward patents that are more similar to the patent under consideration. The backward and forward similarities for each patent is calculated based on the mean of all cosine similarities with the preceding and following patents in the window, using the formula `(x1**a + x2**a + ...)**(1/a)`, with `a` being the exponent. An `a` larger than 1 increases the weight of similarities closer to 1, i.e. of embeddings that are more similar to the one under consideration. The output includes the result for each exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docembedder.analysis import DocAnalysis\n",
    "from docembedder.datamodel import DataModel\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def compute_impacts(embedding_fp, output_dir):\n",
    "    exponents = [1.0, 2.0, 3.0]\n",
    "\n",
    "    impact_novel = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    with DataModel(embedding_fp, read_only=False) as data:\n",
    "        analysis = DocAnalysis(data)\n",
    "       \n",
    "        for window, model in data.iterate_window_models():\n",
    "            results = analysis.impact_novelty_results(window, model, exponents, cache=False, n_jobs=8)\n",
    "\n",
    "            for expon, res in results.items():\n",
    "                if expon == exponents[0]:\n",
    "                    impact_novel[model][\"patent_ids\"].extend(res[\"patent_ids\"])\n",
    "                impact_novel[model][f\"impact-{expon}\"].extend(res[\"impact\"])\n",
    "                impact_novel[model][f\"novelty-{expon}\"].extend(res[\"novelty\"])\n",
    "\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for model, data in impact_novel.items():\n",
    "        classifier_name = model.split(\"-\")[-1]\n",
    "        impact_fp = Path(output_dir, f\"impact-{classifier_name}.csv\")\n",
    "        pd.DataFrame(impact_novel[model]).sort_values(\"patent_ids\").to_csv(impact_fp, index=False)\n",
    "\n",
    "\n",
    "compute_impacts(embedding_fp=output_fp, output_dir=results_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee098a7",
   "metadata": {},
   "source": [
    "### 3.2. Output\n",
    "\n",
    "After the computations are done, novelty and impact scores are written to CSV-files in the results folder. One file per model, with novelty and impact scores for each exponent. The key column refers back to the patent ID's from the original data.\n",
    "\n",
    "Below is a list of the resulting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(path.absolute()) for path in results_fp.iterdir()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
