"""Module containing patent similarity analysis"""

from typing import Sequence, Dict, Union
from pathlib import Path

from scipy import stats
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import dill
import pandas as pd
from numpy import typing as npt

from docembedder.base import BaseDocEmbedder
from docembedder.classification import PatentClassification


class DOCSimilarity:
    """ Class to create similarity and difference matrix

    Arguments
    ---------
    embeddings: numpy.ndarray
        Document vectors generated by BERT/other methods
     """

    def __init__(self, embeddings):
        self.embeddings = embeddings
        self.embeddings_df = pd.DataFrame()
        self.embeddings_df['embeddings'] = self.embeddings.tolist()
        # self.df_patents = pd.read_csv('../data/patents_concatenated.csv')
        self.df_patents = pd.read_csv('data/tst_sample.csv')
        self.df_patents_embeddings = self.df_patents.join(self.embeddings_df, how='left')
        self.forward_block = None
        self.backward_block = None
        self.window_size = 3

    @classmethod
    def from_dill(cls, path="models/document_embeddings_tst.dill"):
        """ Load embeddings to the memory

        Arguments
        ---------
        path: str
            path to the embedding file

        Returns
        -------
        embeddings:
            Initialized embeddings

        """
        with open(path, 'rb') as file:
            embeddings = dill.load(file)
        return cls(embeddings)

    def collect_blocks(self, patent_index):
        """
        Collect a block of patents for an n-year window regarding the year of the focus
        patent.

        """

        # patent_year = self.df_patents_embeddings[
        #     self.df_patents_embeddings['patent'] == patent_number]['year'].values[0]
        grouped_df = self.df_patents_embeddings.groupby('year')
        max_year = max(grouped_df.groups.keys())
        min_year = min(grouped_df.groups.keys())

        patent_year = self.df_patents.loc[patent_index]['year']

        backward_years = patent_year - self.window_size
        backward_years = max(backward_years, min_year)
        forward_years = patent_year + self.window_size
        forward_years = min(forward_years, max_year)

        forward_block_list = []
        backward_block_list = []

        for key in grouped_df.groups.keys():
            if backward_years <= key < patent_year:  # backward n-years patents
                backward_block_sub = grouped_df.get_group(key)
                backward_block_list.append(backward_block_sub)

            if patent_year < key <= forward_years:  # forward n-years patents
                forward_block_sub = grouped_df.get_group(key)
                forward_block_list.append(forward_block_sub)

        if forward_block_list:
            self.forward_block = pd.concat(forward_block_list)
        if backward_block_list:
            self.backward_block = pd.concat(backward_block_list)

    def compute_impact(self, patent_index):
        """ Function to calculate the impact of the focused patent for the period of n-year.
        Impact score is calculated as the average of the backward similarity / the average of
        the forward similarity.
        Arguments
        ----------
        patent_index: int
            The index of focused patent

        """

        backward_similarity = 0
        forward_similarity = 0
        focus_patent_vector = \
            np.array(self.df_patents_embeddings.loc[patent_index]['embeddings'])

        # Calculate backward similarities
        if self.backward_block is not None:

            for bkw in self.backward_block.index:
                backward_similarity += cosine_similarity(
                    [focus_patent_vector],
                    np.array([self.df_patents_embeddings.loc[bkw]['embeddings']])
                )

            average_backward_similarity = backward_similarity / len(self.backward_block)
            average_backward_similarity_list = average_backward_similarity.tolist()
            average_backward_similarity_number = average_backward_similarity_list[0][0]
        else:
            average_backward_similarity_number = None

        # Calculate forward similarities
        if self.forward_block is not None:
            for frw in self.forward_block.index:
                forward_similarity += cosine_similarity(
                    [focus_patent_vector],
                    np.array([self.df_patents_embeddings.loc[frw]['embeddings']])
                )
            average_forward_similarity = forward_similarity / len(self.forward_block)
            average_forward_similarity_list = average_forward_similarity.tolist()
            average_forward_similarity_number = average_forward_similarity_list[0][0]
        else:
            average_forward_similarity_number = None

        # Calculate influence. backwards/forwards
        if (average_backward_similarity_number is not None)\
                & (average_forward_similarity_number is not None):
            self.df_patents_embeddings.loc[patent_index, 'impact'] = \
                average_backward_similarity_number / average_forward_similarity_number

    def compute_novelty(self, patent_index):
        """
        Function for calculating the focused patent's novelty for the period of n-year.
        Novelty score is calculated as the average of the 1- cosine-similarity between Pi and
        patents in the n-backward years

        Arguments
        ----------
        patent_index: int
            The index of focused patent
        """
        backward_dissimilarity = 0
        focus_patent_vector = \
            np.array(self.df_patents_embeddings.loc[patent_index]['embeddings'])

        # Calculate novelty of the focus patent
        if self.backward_block is not None:
            for brow in self.backward_block.index:
                backward_dissimilarity += 1 - cosine_similarity(
                    [focus_patent_vector],
                    np.array([self.df_patents_embeddings.loc[brow]['embeddings']])
                )
            average_backward_similarity = backward_dissimilarity / len(self.backward_block)
            average_backward_similarity_list = average_backward_similarity.tolist()
            self.df_patents_embeddings.loc[patent_index, 'novelty'] = \
                average_backward_similarity_list[0][0]

    def compute_similarity(self):
        """
        Function to compute the novelty score, and impact score of the patents for
        a window size of n-year using cosine similarity.
        """
        for patent_index in range(len(self.df_patents_embeddings)):
            self.collect_blocks(patent_index)
            self.compute_novelty(patent_index)
            self.compute_impact(patent_index)


def get_model_correlations(model: BaseDocEmbedder, documents: Sequence[str]
                           ) -> npt.NDArray[np.float_]:
    """Get all cross correlations of the embeddings for a model

    Arguments
    ---------
    model:
        Model to create the embeddings for.
    documents:
        Patents to encode.

    Returns
    -------
    cross_cor:
        Correlations between all the patents [len(documents, len(documents)].
    """
    model.fit(documents)
    embeddings = model.transform(documents)
    cross_cor = embeddings.dot(embeddings.T)
    return cross_cor


def _sample_single_class(patents: Sequence[str], pat_class: PatentClassification):
    # Somtimes we don't have patent classifications, so try a few times if that is the case.
    n_try = 1000
    for _ in range(n_try):
        i_patent, j_patent = np.random.choice(len(patents), size=2, replace=False)
        try:
            class_cor = pat_class.get_similarity(
                patents[i_patent]["patent"],
                patents[j_patent]["patent"])
            return i_patent, j_patent, class_cor
        except ValueError:
            pass

    raise ValueError("Cannot find patents with classification.")


def sample_class_correlation(patents: Sequence[str],
                             cross_correlations: Dict[str, npt.NDArray[np.float_]],
                             class_fp: Union[str, Path]=Path("..", "data", "GPCPCs.txt"),
                             n_sample: int=10000) -> Dict[str, float]:
    """Compute the performance of models using patent classifications

    It does so by sampling the performance, since computing the full classification
    correlations can be very time consuming.

    Arguments
    ---------
    cross_correlations:
        For each model, this contains a cross correlation/similarity matrix.
    class_fp:
        Filename for the patent classifications.
    n_sample:
        Number of samples for benchmarking purposes. Higher means longer running times
        but better accuracy (at least up to sampling the whole matrix).
    """
    pat_class = PatentClassification(class_fp)
    sampled_correlations = {model_name: np.zeros(n_sample) for model_name in cross_correlations}
    class_correlations = np.zeros(n_sample)
    for i_sample in range(n_sample):
        i_patent, j_patent, class_cor = _sample_single_class(patents, pat_class)
        for model_name in cross_correlations:
            sampled_correlations[model_name][i_sample] = cross_correlations[
                model_name][i_patent, j_patent]
        class_correlations[i_sample] = class_cor

    model_correlations = {model_name: stats.spearmanr(class_correlations, model_cor).correlation
                          for model_name, model_cor in sampled_correlations.items()}
    return model_correlations
