"""Module containing patent similarity analysis"""

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances
import numpy as np
import dill
import pandas as pd


class DOCSimilarity:
    """ Class to create similarity and difference matrix

    Arguments
    ---------
    embeddings: numpy.ndarray
        Document vectors generated by BERT/other methods
     """

    def __init__(self, embeddings):
        self.embeddings = embeddings

    @classmethod
    def from_dill(cls, path="../models/document_embeddings.dill"):
        """ Load embeddings to the memory

        Arguments
        ---------
        path: str
            path to the embedding file

        """
        with open(path, 'rb') as file:
            embeddings = dill.load(file)
        return cls(embeddings)

    def cosine_similarity_matrix(self):
        """create similarity matrix
        """
        pairwise_similarities = cosine_similarity(self.embeddings)
        return pairwise_similarities

    def euclidean_similarity_matrix(self):
        """ Create difference matrix
        """
        pairwise_similarities = 1-euclidean_distances(self.embeddings)
        return pairwise_similarities

    @staticmethod
    def most_similar(documents, document_id, similarity_matrix):
        """ Find most similar documents

        Arguments
        --------
        documents: Pandas.DataFrame
            The entire corpus
        document_id: int
            The document of interest
        similarity_matrix:  numpy.ndarray
            Euclidean or cosine similarity matrix
        """
        print(f'Document: {documents.iloc[document_id]["contents"]}')
        print('\n')
        print('Most similar Document:')

        similar_index = np.argsort(similarity_matrix[document_id])[-2::]

        for index in similar_index:
            if index == document_id:
                continue
            most_similar_documents = documents.iloc[index]["contents"]
            most_similar_document_index = similarity_matrix[document_id][index]
            return most_similar_documents, most_similar_document_index
            # print('\n')
            # print(f'Document: {documents.iloc[index]["contents"]}')
            # print(f'{measure} : {similarity_matrix[doc_id][index]}')

    @staticmethod
    def collect_blocks(patent_id, look_up_window):
        """
        Collect a block of patents for a window of n years regarding the year of the focus
        patent.

        """

        df_patents = pd.read_csv('../data/patents_concatenated.csv')
        patent_year = df_patents[df_patents['patent'] == patent_id]['year'].values[0]
        print(patent_year)

        backward_years = patent_year - look_up_window
        forward_years = patent_year + look_up_window

        forward_block_list = []
        backward_block_list = []

        grouped_df = df_patents.groupby('year')

        for key, item in grouped_df:
            if backward_years <= key <= patent_year:  # backward n-years patents
                backward_block_sub = grouped_df.get_group(key)
                backward_block_list.append(backward_block_sub)

            if patent_year <= key <= forward_years:  # forward n-years patents
                forward_block_sub = grouped_df.get_group(key)
                forward_block_list.append(forward_block_sub)
        forward_block = pd.concat(forward_block_list)
        backward_block = pd.concat(backward_block_list)

        return backward_block, forward_block


if __name__ == "__main__":
    patent_analyser = DOCSimilarity.from_dill()
    a, b = patent_analyser.collect_blocks(patent_id=2445033, look_up_window=3)
    # print(a)
    print('#########')
    print(b)
